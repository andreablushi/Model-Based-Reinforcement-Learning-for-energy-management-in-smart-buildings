name: "sac"

normalize: true
normalize_double_precision: true
target_is_delta: true
sac_samples_action: true
learned_rewards: true
initial_exploration_steps: 719
random_initial_explore: false

num_eval_episodes: 1
eval_freq: 1

#Periodic network reset of SAC Agent
critic_reset: false
#critic_reset_factor = crf, only has effect if critic_reset = true
# new_weights = (1-crf)*old_weights+crf*new_weights
critic_reset_factor: 1.0
critic_reset_every_step: 20000 # env_steps

# --------------------------------------------
#          SAC Agent configuration
# --------------------------------------------
agent:
  num_inputs: ???
  action_space:
    low: ???
    high: ???
    shape: ???
  args:
    layernorm: true 
    gamma: ${overrides.sac_gamma}
    tau: ${overrides.sac_tau}
    alpha: ${overrides.sac_alpha}
    policy: ${overrides.sac_policy}
    target_update_interval: ${overrides.sac_target_update_interval}
    automatic_entropy_tuning: ${overrides.sac_automatic_entropy_tuning}
    target_entropy: ${overrides.sac_target_entropy}
    hidden_size: ${overrides.sac_hidden_size}
    device: ${device}
    lr: ${overrides.sac_lr}
    wd: ${overrides.sac_wd}
    actor_lr_factor: ${overrides.sac_actor_lr_factor}
    start_steps: ${overrides.sac_start_steps}
    num_steps: ${overrides.num_steps}
    replay_size: ${overrides.sac_replay_size}
    batch_size: ${overrides.sac_batch_size}
    eval: true

